{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train polyphony encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tensorflow warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# 0 = all messages are logged (default behavior)\n",
    "# 1 = INFO messages are not printed\n",
    "# 2 = INFO and WARNING messages are not printed\n",
    "# 3 = INFO, WARNING, and ERROR messages are not printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from mukkeBude.model import MukkeBudeTransformer\n",
    "from mukkeBude.mapping import MusicMapping\n",
    "import mukkeBude.utils as utils\n",
    "import music21 as m21\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# Check if GPU is found\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings\n",
    "mapping = MusicMapping.create()\n",
    "\n",
    "# optional save the mapping\n",
    "# mapping.save(\"mapping.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the music21 corpus for the bach training data. </br>\n",
    "You can adjust the `paths` to reduce the number of training songs. </br>\n",
    "</br>\n",
    "See: https://web.mit.edu/music21/doc/about/referenceCorpus.html\n",
    "\n",
    "To load custom training data use:\n",
    "```python\n",
    "# Load songs\n",
    "from pathlib import Path\n",
    "\n",
    "paths = list(Path(\"./dataset/Pokemon\").rglob(\"*.midi\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 songs in corpus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flori\\anaconda3\\envs\\tf\\lib\\site-packages\\mukkeBude\\utils.py:341: StreamIteratorInefficientWarning: highestTime is not defined on StreamIterators. Call .stream() first for efficiency\n",
      "  song.flat.getElementsByClass(\"Note\").highestTime,\n",
      "c:\\Users\\flori\\anaconda3\\envs\\tf\\lib\\site-packages\\mukkeBude\\utils.py:342: StreamIteratorInefficientWarning: highestTime is not defined on StreamIterators. Call .stream() first for efficiency\n",
      "  song.flat.getElementsByClass(\"Chord\").highestTime,\n",
      "c:\\Users\\flori\\anaconda3\\envs\\tf\\lib\\site-packages\\mukkeBude\\utils.py:341: StreamIteratorInefficientWarning: highestTime is not defined on StreamIterators. Call .stream() first for efficiency\n",
      "  song.flat.getElementsByClass(\"Note\").highestTime,\n",
      "c:\\Users\\flori\\anaconda3\\envs\\tf\\lib\\site-packages\\mukkeBude\\utils.py:342: StreamIteratorInefficientWarning: highestTime is not defined on StreamIterators. Call .stream() first for efficiency\n",
      "  song.flat.getElementsByClass(\"Chord\").highestTime,\n",
      "In c:\\Users\\flori\\anaconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\ggplot.mplstyle: highestTime is not defined on StreamIterators. Call .stream() first for efficiency\n",
      "In c:\\Users\\flori\\anaconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\ggplot.mplstyle: highestTime is not defined on StreamIterators. Call .stream() first for efficiency\n",
      "c:\\Users\\flori\\anaconda3\\envs\\tf\\lib\\site-packages\\mukkeBude\\utils.py:341: StreamIteratorInefficientWarning: highestTime is not defined on StreamIterators. Call .stream() first for efficiency\n",
      "  song.flat.getElementsByClass(\"Note\").highestTime,\n",
      "c:\\Users\\flori\\anaconda3\\envs\\tf\\lib\\site-packages\\mukkeBude\\utils.py:342: StreamIteratorInefficientWarning: highestTime is not defined on StreamIterators. Call .stream() first for efficiency\n",
      "  song.flat.getElementsByClass(\"Chord\").highestTime,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs encoded: 16\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "paths = list(Path(\"../mukkeBude/songs/pokemon\").rglob(\"*.mid\"))\n",
    "# Load songs\n",
    "# paths = m21.corpus.getComposer('bach')\n",
    "# paths = paths[:100]\n",
    "print(f\"Found {len(paths)} songs in corpus.\")\n",
    "\n",
    "encoded_songs = []\n",
    "for path in paths:\n",
    "    song = utils.read_single(path)\n",
    "    encoded_song = utils.to_polyphonic_encoding(song, mapping)\n",
    "    encoded_songs.append(mapping.textify(encoded_song))\n",
    "\n",
    "print(f\"Songs encoded: {len(encoded_songs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "utils.create_train_data(encoded_songs, \"raw_train_ds.txt\")\n",
    "print(\"Dataset created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, None, 256)        780288    \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " transformer_decoder (Transf  (None, None, 256)        394749    \n",
      " ormerDecoder)                                                   \n",
      "                                                                 \n",
      " transformer_decoder_1 (Tran  (None, None, 256)        394749    \n",
      " sformerDecoder)                                                 \n",
      "                                                                 \n",
      " transformer_decoder_2 (Tran  (None, None, 256)        394749    \n",
      " sformerDecoder)                                                 \n",
      "                                                                 \n",
      " transformer_decoder_3 (Tran  (None, None, 256)        394749    \n",
      " sformerDecoder)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 1000)        257000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,616,284\n",
      "Trainable params: 2,616,284\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1/1 - 6s - loss: 7.1133 - perplexity: 1228.2037 - 6s/epoch - 6s/step\n",
      "Epoch 2/50\n",
      "1/1 - 0s - loss: 5.0518 - perplexity: 156.3055 - 83ms/epoch - 83ms/step\n",
      "Epoch 3/50\n",
      "1/1 - 0s - loss: 4.1227 - perplexity: 61.7230 - 85ms/epoch - 85ms/step\n",
      "Epoch 4/50\n",
      "1/1 - 0s - loss: 3.6984 - perplexity: 40.3808 - 85ms/epoch - 85ms/step\n",
      "Epoch 5/50\n",
      "1/1 - 0s - loss: 3.4072 - perplexity: 30.1801 - 74ms/epoch - 74ms/step\n",
      "Epoch 6/50\n",
      "1/1 - 0s - loss: 3.1455 - perplexity: 23.2305 - 89ms/epoch - 89ms/step\n",
      "Epoch 7/50\n",
      "1/1 - 0s - loss: 2.9130 - perplexity: 18.4116 - 89ms/epoch - 89ms/step\n",
      "Epoch 8/50\n",
      "1/1 - 0s - loss: 2.7140 - perplexity: 15.0901 - 89ms/epoch - 89ms/step\n",
      "Epoch 9/50\n",
      "1/1 - 0s - loss: 2.5435 - perplexity: 12.7239 - 89ms/epoch - 89ms/step\n",
      "Epoch 10/50\n",
      "1/1 - 0s - loss: 2.4046 - perplexity: 11.0740 - 76ms/epoch - 76ms/step\n",
      "Epoch 11/50\n",
      "1/1 - 0s - loss: 2.2888 - perplexity: 9.8632 - 73ms/epoch - 73ms/step\n",
      "Epoch 12/50\n",
      "1/1 - 0s - loss: 2.1939 - perplexity: 8.9700 - 95ms/epoch - 95ms/step\n",
      "Epoch 13/50\n",
      "1/1 - 0s - loss: 2.1184 - perplexity: 8.3175 - 89ms/epoch - 89ms/step\n",
      "Epoch 14/50\n",
      "1/1 - 0s - loss: 2.0465 - perplexity: 7.7410 - 81ms/epoch - 81ms/step\n",
      "Epoch 15/50\n",
      "1/1 - 0s - loss: 1.9799 - perplexity: 7.2421 - 88ms/epoch - 88ms/step\n",
      "Epoch 16/50\n",
      "1/1 - 0s - loss: 1.9208 - perplexity: 6.8264 - 86ms/epoch - 86ms/step\n",
      "Epoch 17/50\n",
      "1/1 - 0s - loss: 1.8633 - perplexity: 6.4447 - 76ms/epoch - 76ms/step\n",
      "Epoch 18/50\n",
      "1/1 - 0s - loss: 1.8079 - perplexity: 6.0973 - 74ms/epoch - 74ms/step\n",
      "Epoch 19/50\n",
      "1/1 - 0s - loss: 1.7537 - perplexity: 5.7759 - 89ms/epoch - 89ms/step\n",
      "Epoch 20/50\n",
      "1/1 - 0s - loss: 1.6963 - perplexity: 5.4539 - 90ms/epoch - 90ms/step\n",
      "Epoch 21/50\n",
      "1/1 - 0s - loss: 1.6404 - perplexity: 5.1571 - 90ms/epoch - 90ms/step\n",
      "Epoch 22/50\n",
      "1/1 - 0s - loss: 1.5821 - perplexity: 4.8652 - 91ms/epoch - 91ms/step\n",
      "Epoch 23/50\n",
      "1/1 - 0s - loss: 1.5086 - perplexity: 4.5205 - 94ms/epoch - 94ms/step\n",
      "Epoch 24/50\n",
      "1/1 - 0s - loss: 1.4459 - perplexity: 4.2457 - 91ms/epoch - 91ms/step\n",
      "Epoch 25/50\n",
      "1/1 - 0s - loss: 1.3845 - perplexity: 3.9930 - 74ms/epoch - 74ms/step\n",
      "Epoch 26/50\n",
      "1/1 - 0s - loss: 1.3153 - perplexity: 3.7260 - 87ms/epoch - 87ms/step\n",
      "Epoch 27/50\n",
      "1/1 - 0s - loss: 1.2531 - perplexity: 3.5012 - 90ms/epoch - 90ms/step\n",
      "Epoch 28/50\n",
      "1/1 - 0s - loss: 1.1989 - perplexity: 3.3166 - 93ms/epoch - 93ms/step\n",
      "Epoch 29/50\n",
      "1/1 - 0s - loss: 1.1365 - perplexity: 3.1158 - 88ms/epoch - 88ms/step\n",
      "Epoch 30/50\n",
      "1/1 - 0s - loss: 1.0838 - perplexity: 2.9557 - 91ms/epoch - 91ms/step\n",
      "Epoch 31/50\n",
      "1/1 - 0s - loss: 1.0385 - perplexity: 2.8251 - 94ms/epoch - 94ms/step\n",
      "Epoch 32/50\n",
      "1/1 - 0s - loss: 0.9897 - perplexity: 2.6906 - 75ms/epoch - 75ms/step\n",
      "Epoch 33/50\n",
      "1/1 - 0s - loss: 0.9345 - perplexity: 2.5461 - 73ms/epoch - 73ms/step\n",
      "Epoch 34/50\n",
      "1/1 - 0s - loss: 0.8842 - perplexity: 2.4212 - 80ms/epoch - 80ms/step\n",
      "Epoch 35/50\n",
      "1/1 - 0s - loss: 0.8412 - perplexity: 2.3192 - 85ms/epoch - 85ms/step\n",
      "Epoch 36/50\n",
      "1/1 - 0s - loss: 0.7980 - perplexity: 2.2210 - 83ms/epoch - 83ms/step\n",
      "Epoch 37/50\n",
      "1/1 - 0s - loss: 0.7555 - perplexity: 2.1286 - 85ms/epoch - 85ms/step\n",
      "Epoch 38/50\n",
      "1/1 - 0s - loss: 0.7089 - perplexity: 2.0318 - 81ms/epoch - 81ms/step\n",
      "Epoch 39/50\n",
      "1/1 - 0s - loss: 0.6686 - perplexity: 1.9516 - 81ms/epoch - 81ms/step\n",
      "Epoch 40/50\n",
      "1/1 - 0s - loss: 0.6376 - perplexity: 1.8919 - 86ms/epoch - 86ms/step\n",
      "Epoch 41/50\n",
      "1/1 - 0s - loss: 0.6015 - perplexity: 1.8249 - 79ms/epoch - 79ms/step\n",
      "Epoch 42/50\n",
      "1/1 - 0s - loss: 0.5635 - perplexity: 1.7569 - 81ms/epoch - 81ms/step\n",
      "Epoch 43/50\n",
      "1/1 - 0s - loss: 0.5303 - perplexity: 1.6995 - 82ms/epoch - 82ms/step\n",
      "Epoch 44/50\n",
      "1/1 - 0s - loss: 0.4966 - perplexity: 1.6432 - 83ms/epoch - 83ms/step\n",
      "Epoch 45/50\n",
      "1/1 - 0s - loss: 0.4639 - perplexity: 1.5902 - 79ms/epoch - 79ms/step\n",
      "Epoch 46/50\n",
      "1/1 - 0s - loss: 0.4333 - perplexity: 1.5423 - 86ms/epoch - 86ms/step\n",
      "Epoch 47/50\n",
      "1/1 - 0s - loss: 0.4076 - perplexity: 1.5032 - 73ms/epoch - 73ms/step\n",
      "Epoch 48/50\n",
      "1/1 - 0s - loss: 0.3974 - perplexity: 1.4880 - 89ms/epoch - 89ms/step\n",
      "Epoch 49/50\n",
      "1/1 - 0s - loss: 0.4400 - perplexity: 1.5527 - 85ms/epoch - 85ms/step\n",
      "Epoch 50/50\n",
      "1/1 - 0s - loss: 0.4546 - perplexity: 1.5755 - 80ms/epoch - 80ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x196263d0370>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model = MukkeBudeTransformer(mapping)\n",
    "print(model)\n",
    "\n",
    "logdir = \"logs/bach_transformer\"\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.train(\"raw_train_ds.txt\", min_training_seq_len=32, epochs=50, tensorboard_callback=tensorboard_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-87d85ad84596d5e5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-87d85ad84596d5e5\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/bach_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\flori\\\\anaconda3\\\\envs\\\\tf\\\\lib\\\\site-packages\\\\mukkeBude\\\\model\\\\preTrainedModels\\\\Videospielmusik_polyphonie_transformer.h5'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(\"Videospielmusik_polyphonie_transformer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate polyphony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tensorflow warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# 0 = all messages are logged (default behavior)\n",
    "# 1 = INFO messages are not printed\n",
    "# 2 = INFO and WARNING messages are not printed\n",
    "# 3 = INFO, WARNING, and ERROR messages are not printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from mukkeBude.mapping import MusicMapping\n",
    "import mukkeBude.utils as utils\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from mukkeBude.mapping import SPECIAL_TOKS\n",
    "from mukkeBude.mapping import SEP\n",
    "from mukkeBude.mapping import BOS\n",
    "\n",
    "# Check if GPU is found\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings\n",
    "mapping = MusicMapping.create()\n",
    "\n",
    "# optional save the mapping\n",
    "# mapping.save(\"mapping.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This does not work! In the current state you can not load a transformer network from a file!**</br>\n",
    "You need to train it first and use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = MukkeBudeTransformer.load(mapping, \"Bach_polyphonie_transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Create song\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m generated_song \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m\"\u001b[39;49m\u001b[39mn89 d2 n34 d2 n22 d2 xxsep d2 n87 d2 n53 d2 xxsep d2 n86 d2 n62 d2 n58 d2 xxsep d2 n84 d2 n53 d2 xxsep d2 n94 d2 n65 d2 n62 d2 n58 d2 xxsep d2 n91 d2 n53 d2 xxsep d2 n93 d2 n63 d2 n60 d2 n57 d2 xxsep d2 n91 d2 n53 d2 xxsep d2 n89 d4 n34 d2 n22 d2 xxsep d2 n53 d2 xxsep d2 n62 d2 n58 d2 xxsep d2 n86 d2 n53 d2 xxsep d2 n82 d2 n65 d2 n62 d2 n58 d2 xxsep d2 n82 d2 n53 d2 xxsep d2 n84 d2 n62 d2 n58 d2 xxsep d2 n86 d2 n53 d2 xxsep d2 n87 d3 n39 d2 n27 d2 xxsep d2 n45 d2 xxsep d1 n63 d1 xxsep d1 n65 d1 n53 d2 n51 d2 n48 d2 xxsep d1 n69 d1 xxsep d1 n72 d1 n45 d2 xxsep d1 n75 d1 xxsep d1 n77 d2 n57 d2 n53 d2 n51 d2 n48 d2 xxsep d2 n81 d2 n45 d2 xxsep d2 n82 d2 n53 d2 n51 d2 n48 d2 xxsep d2 n84 d2 n45 d2 xxsep d2 n86 d2 n34 d4 n22 d4 xxsep d2 n82 d2 xxsep d2 n89 d2 n62 d4 n58 d4 n53 d4 xxsep d2 n87 d1 xxsep d1\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_length\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, probability\u001b[39m=\u001b[39;49m\u001b[39m0.4\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Remove REST and WAIT_LSTM from SPECIAL_TOKS\u001b[39;00m\n\u001b[0;32m      5\u001b[0m special_tokens \u001b[39m=\u001b[39m SPECIAL_TOKS\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\flori\\anaconda3\\envs\\tf\\lib\\site-packages\\mukkeBude\\model\\mukke_bude_transformer.py:104\u001b[0m, in \u001b[0;36mMukkeBudeTransformer.generate\u001b[1;34m(self, start_seed, max_length, probability)\u001b[0m\n\u001b[0;32m    102\u001b[0m prompt_ids \u001b[39m=\u001b[39m []\n\u001b[0;32m    103\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m--> 104\u001b[0m     prompt_ids\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mtoken_to_id(prompt))\n\u001b[0;32m    105\u001b[0m prompt_tokens \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(prompt_ids)\n\u001b[0;32m    106\u001b[0m \u001b[39m# prompt_tokens = tf.convert_to_tensor([self.tokenizer.token_to_id(start_seed)])\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\flori\\anaconda3\\envs\\tf\\lib\\site-packages\\keras_nlp\\tokenizers\\word_piece_tokenizer.py:368\u001b[0m, in \u001b[0;36mWordPieceTokenizer.token_to_id\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Convert a string token to an integer id.\"\"\"\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[39m# This will be slow, but keep memory usage down compared to building a\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[39m# . Assuming the main use case is looking up a few special tokens\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39m# early in the vocab, this should be fine.\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocabulary\u001b[39m.\u001b[39;49mindex(token)\n",
      "File \u001b[1;32mc:\\Users\\flori\\anaconda3\\envs\\tf\\lib\\_collections_abc.py:1080\u001b[0m, in \u001b[0;36mSequence.index\u001b[1;34m(self, value, start, stop)\u001b[0m\n\u001b[0;32m   1078\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1079\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1080\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create song\n",
    "generated_song = model.generate(\"n46 d4 xxsep d4 n53 d2 n50 d2 xxsep d2 n62 d1 n58 d1 xxsep d2 n53 d2 n53 d2 n50 d2 xxsep d4 n46 d4 xxsep d4 n55 d2 n51 d2 xxsep d2 n63 d1 n58 d1 xxsep d2 n55 d2 n55 d2 n51 d2 xxsep d4 n46 d4 xxsep d4 n57 d2 n53 d2 xxsep d2 n65 d1 n60 d1 xxsep d2 n57 d2 n57 d2 n53 d2 xxsep d4 n46 d4 xxsep d4 n55 d2 n51 d2 xxsep d2 n63 d1 n58 d1 xxsep d2 n55 d2 n55 d2 n51 d2 xxsep d4 n77 d2 n74 d2 n46 d4 xxsep d4 n75 d1 n72 d1 n53 d2 n50 d2 xxsep d2 n74 d2 n70 d2 xxsep d2 n53 d2 n50 d2 xxsep d2 n75 d1 n72 d1 xxsep d2 n46 d4 xxsep d2 n74 d2 n70 d2 xxsep d2 n53 d2 n50 d2 xxsep d2 n70 d1 n65 d1 xxsep d2 n65 d2 n62 d2 n53 d2 n50 d2 xxsep d4 n67 d2 n63 d2 n46 d4 xxsep d4 n65 d1 n62 d1 n55 d2 n51 d2 xxsep d2 n67 d2 n63 d2 xxsep d2 n55 d2 n51 d2 xxsep d2 n69 d1 n60 d1 xxsep d2 n70 d1 n62 d1 n46 d4 xxsep d2 n77 d1 n70 d1 xxsep d2 n53 d2 n50 d2 xxsep d4 n53 d2 n50 d2 xxsep d3 n77 d1 n75 d1 xxsep d1 n79 d2 n75 d2 n46 d4 xxsep d4 n77 d1 n74 d1 n55 d2 n51 d2 xxsep d2 n79 d2 n75 d2 xxsep d2 n55 d2 n51 d2 xxsep d2 n81 d1 n72 d1 xxsep d2 n82 d2 n74 d2 n46 d4 xxsep d4 n77 d1 n70 d2 n53 d2 n50 d2 xxsep d2 n65 d2 xxsep d2 n53 d2 n50 d2 xxsep d2 n62 d1 xxsep d2 n75 d2 n39 d4 xxsep d4 n74 d1 n62 d1 n51 d2 n46 d2 xxsep d2 n72 d2 n63 d2 xxsep d2 n51 d2 n46 d2 xxsep d2 n70 d1 n64 d1 n52 d2 xxsep d2 n79 d2 n65 d2 n41 d4 xxsep d4 n69 d1 n53 d2 n48 d2 xxsep d2 n77 d1 n72 d2 xxsep d2 n75 d1 n53 d2 n48 d2 xxsep d2 n74 d1 n65 d1 xxsep d1 n72 d1 xxsep d1 n70 d2 n67 d2 n63 d2 n51 d4 n39 d4 xxsep d4 n58 d1 n55 d2 n51 d2 n46 d2 xxsep d2 n70 d2 n67 d2 n63 d2 xxsep d2 n55 d2 n51 d2 n46 d2 xxsep d2 n58 d1 xxsep d2 n70 d2 n65 d2 n62 d2 n46 d4 n34 d4 xxsep d4 n58 d1 n58 d2 n53 d2 n50 d2 xxsep d2 n70 d2 n65 d2 n62 d2 xxsep d2 n58 d2 n53 d2 n50 d2 xxsep d2 n58 d1 xxsep d2 n77 d2 n69 d2 n53 d4 n41 d4 xxsep d4 n75 d1 n57 d2 n53 d2 n48 d2 xxsep d2 n74 d2 n65 d2 xxsep d2 n57 d2 n53 d2 n48 d2 xxsep d2 n72 d1 xxsep d2\", max_length=1000, probability=0.4)\n",
    "\n",
    "# Remove REST and WAIT_LSTM from SPECIAL_TOKS\n",
    "special_tokens = SPECIAL_TOKS.copy()\n",
    "special_tokens.remove(SEP)\n",
    "special_tokens.remove(BOS)\n",
    "\n",
    "generated_song = \" \".join(utils.replace_special_tokens(generated_song.split(), \"d1\", special_tokens))\n",
    "print(generated_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to music21\n",
    "new_song_ints = mapping.numericalize(generated_song.split(\" \"))\n",
    "new_song_ints = np.array(new_song_ints)\n",
    "\n",
    "new_song = utils.from_polyphonic_encoding(new_song_ints, mapping, bpm=140)\n",
    "\n",
    "path = Path(\"generated_song_pokemon_trans_poly.midi\")\n",
    "utils.write_midi(new_song, path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
