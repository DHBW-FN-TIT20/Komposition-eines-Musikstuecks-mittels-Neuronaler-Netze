{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n",
    "!echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\mukkeBude\\utils.py:108: StreamIteratorInefficientWarning: highestTime is not defined on StreamIterators. Call .stream() first for efficiency\n",
      "  highest_time = max(song.flat.getElementsByClass('Note').highestTime, song.flat.getElementsByClass('Chord').highestTime)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1  69 ... 136  45 136]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at transfo-xl-wt103 were not used when initializing TFTransfoXLModel: ['crit']\n",
      "- This IS expected if you are initializing TFTransfoXLModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFTransfoXLModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFTransfoXLModel were initialized from the model checkpoint at transfo-xl-wt103.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFTransfoXLModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/deherkfl/.cache/huggingface/datasets/json/default-c9c0a2532aa339b9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1008.00it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 124.98it/s]\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/deherkfl/.cache/huggingface/datasets/json/default-c9c0a2532aa339b9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 58.81it/s]\n",
      "Parameter 'function'=<bound method MukkeBudeTransformer._preprocess_dataset of <model.mukke_bude_transformer.MukkeBudeTransformer object at 0x000002097034D550>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'labels', 'input_ids', 'label'],\n",
      "        num_rows: 1286\n",
      "    })\n",
      "})\n",
      "[  0   1  69 136  69 136  64 136  61 136  57 136   3 136  76 136  71 134\n",
      "  64 136  59 136  56 136   3 134  64 134   3 134  73 136]\n",
      "[[3723]]\n",
      "[192, 194, 65, 3723, 12375, 3723, 12375, 2194, 12375, 3373, 12375, 3245, 12375, 92, 12375, 2971, 12375, 3854, 12281, 2194, 12375, 3335, 12375, 2811, 12375, 92, 12281, 2194, 12281, 92, 12281, 3761, 12375, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[  1  69 136  69 136  64 136  61 136  57 136   3 136  76 136  71 134  64\n",
      " 136  59 136  56 136   3 134  64 134   3 134  73 136  69]\n",
      "[[12281]]\n",
      "[192, 65, 3723, 12375, 3723, 12375, 2194, 12375, 3373, 12375, 3245, 12375, 92, 12375, 2971, 12375, 3854, 12281, 2194, 12375, 3335, 12375, 2811, 12375, 92, 12281, 2194, 12281, 92, 12281, 3761, 12375, 3723, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Model: \"tf_transfo_xl_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFTransfoXLMai  multiple                 283885936 \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 283,885,936\n",
      "Trainable params: 283,885,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\transformers\\trainer_tf.py:115: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"d:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\transformers\\trainer_tf.py\", line 711, in distributed_training_steps  *\n        self.args.strategy.run(self.apply_gradients, inputs)\n    File \"d:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\transformers\\trainer_tf.py\", line 653, in apply_gradients  *\n        gradients = self.training_step(features, labels, nb_instances_in_global_batch)\n    File \"d:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\transformers\\trainer_tf.py\", line 636, in training_step  *\n        per_example_loss, _ = self.run_model(features, labels, True)\n    File \"d:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\transformers\\trainer_tf.py\", line 758, in run_model  *\n        outputs = self.model(features, labels=labels, training=training)[:2]\n    File \"d:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 903, in run_call_with_unpacked_inputs  *\n        unpacked_inputs = input_processing(func, config, **fn_args_and_kwargs)\n    File \"d:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 499, in input_processing  *\n        raise ValueError(\n\n    ValueError: The following keyword arguments are not supported by this model: ['labels'].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m data \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mcreate_train_data(encoded_song)\n\u001b[0;32m     13\u001b[0m \u001b[39m# train = utils.split_to_training_data(mapping.textify(encoded_song))\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m model\u001b[39m.\u001b[39;49mtrain(data, \u001b[39mlen\u001b[39;49m(mapping))\n",
      "File \u001b[1;32md:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\mukkeBude\\model\\mukke_bude_transformer.py:49\u001b[0m, in \u001b[0;36mMukkeBudeTransformer.train\u001b[1;34m(self, data, batch_size)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m# with training_args.strategy.scope():\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m#     model = TFTransfoXLModel.from_pretrained(\"transfo-xl-wt103\")\u001b[39;00m\n\u001b[0;32m     43\u001b[0m trainer \u001b[39m=\u001b[39m TFTrainer(\n\u001b[0;32m     44\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[0;32m     45\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m     46\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset\n\u001b[0;32m     47\u001b[0m )\n\u001b[1;32m---> 49\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32md:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\transformers\\trainer_tf.py:569\u001b[0m, in \u001b[0;36mTFTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    566\u001b[0m     steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    567\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 569\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistributed_training_steps(batch)\n\u001b[0;32m    571\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m=\u001b[39m iterations\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    572\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_logging \u001b[39m=\u001b[39m epoch_iter \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps_per_epoch\n",
      "File \u001b[1;32md:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filec6erm7hb.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__distributed_training_steps\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m nb_instances_in_batch \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_compute_nb_instances, (ag__\u001b[39m.\u001b[39mld(batch),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     10\u001b[0m inputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_get_step_inputs, (ag__\u001b[39m.\u001b[39mld(batch), ag__\u001b[39m.\u001b[39mld(nb_instances_in_batch)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 11\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mrun, (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mapply_gradients, ag__\u001b[39m.\u001b[39mld(inputs)), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filex35wto3v.py:111\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__apply_gradients\u001b[1;34m(self, features, labels, nb_instances_in_global_batch)\u001b[0m\n\u001b[0;32m    109\u001b[0m reduced_labels \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mreduced_labels\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    110\u001b[0m _ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 111\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m, if_body_4, else_body_4, get_state_5, set_state_5, (\u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filex35wto3v.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__apply_gradients.<locals>.if_body_4\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mif_body_4\u001b[39m():\n\u001b[0;32m     17\u001b[0m     \u001b[39mnonlocal\u001b[39;00m features, labels\n\u001b[1;32m---> 18\u001b[0m     gradients \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mtraining_step, (ag__\u001b[39m.\u001b[39;49mld(features), ag__\u001b[39m.\u001b[39;49mld(labels), ag__\u001b[39m.\u001b[39;49mld(nb_instances_in_global_batch)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[0;32m     19\u001b[0m     ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mapply_gradients, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mlist\u001b[39m), (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mzip\u001b[39m), (ag__\u001b[39m.\u001b[39mld(gradients), ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrainable_variables), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filezxt1kddi.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__training_step\u001b[1;34m(self, features, labels, nb_instances_in_global_batch)\u001b[0m\n\u001b[0;32m     13\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     14\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 15\u001b[0m (per_example_loss, _) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mrun_model, (ag__\u001b[39m.\u001b[39mld(features), ag__\u001b[39m.\u001b[39mld(labels), \u001b[39mTrue\u001b[39;00m), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m scaled_loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(per_example_loss) \u001b[39m/\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mcast, (ag__\u001b[39m.\u001b[39mld(nb_instances_in_global_batch),), \u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(per_example_loss)\u001b[39m.\u001b[39mdtype), fscope)\n\u001b[0;32m     17\u001b[0m gradients \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mgradients, (ag__\u001b[39m.\u001b[39mld(scaled_loss), ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrainable_variables), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filefgzfstqe.py:52\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_model\u001b[1;34m(self, features, labels, training)\u001b[0m\n\u001b[0;32m     50\u001b[0m     outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mmodel, (ag__\u001b[39m.\u001b[39mld(features),), \u001b[39mdict\u001b[39m(labels\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(labels), training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)[:\u001b[39m2\u001b[39m]\n\u001b[0;32m     51\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39misinstance\u001b[39m), (ag__\u001b[39m.\u001b[39mld(labels), ag__\u001b[39m.\u001b[39mld(\u001b[39mdict\u001b[39m)), \u001b[39mNone\u001b[39;00m, fscope), if_body_1, else_body_1, get_state_1, set_state_1, (\u001b[39m'\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[0;32m     53\u001b[0m (loss, logits) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(outputs)[:\u001b[39m2\u001b[39m]\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_2\u001b[39m():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filefgzfstqe.py:50\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_model.<locals>.else_body_1\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39melse_body_1\u001b[39m():\n\u001b[0;32m     49\u001b[0m     \u001b[39mnonlocal\u001b[39;00m outputs\n\u001b[1;32m---> 50\u001b[0m     outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mmodel, (ag__\u001b[39m.\u001b[39;49mld(features),), \u001b[39mdict\u001b[39;49m(labels\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(labels), training\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(training)), fscope)[:\u001b[39m2\u001b[39m]\n",
      "File \u001b[1;32md:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileb91734f2.py:33\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m config \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(\u001b[39m'\u001b[39m\u001b[39mEncoderDecoder\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(input_processing), (ag__\u001b[39m.\u001b[39mld(func), ag__\u001b[39m.\u001b[39mld(config)), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(fn_args_and_kwargs)), fscope)\n\u001b[0;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileih5hpr1i.py:113\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__input_processing\u001b[1;34m(func, config, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m     ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mlen\u001b[39m), (ag__\u001b[39m.\u001b[39mld(kwargs)[\u001b[39m'\u001b[39m\u001b[39mkwargs_call\u001b[39m\u001b[39m'\u001b[39m],), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, if_body_4, else_body_4, get_state_4, set_state_4, (), \u001b[39m0\u001b[39m)\n\u001b[0;32m    112\u001b[0m     ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(kwargs)\u001b[39m.\u001b[39mpop, (\u001b[39m'\u001b[39m\u001b[39mkwargs_call\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m--> 113\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(has_kwargs), if_body_5, else_body_5, get_state_5, set_state_5, (\u001b[39m\"\u001b[39m\u001b[39moutput[\u001b[39m\u001b[39m'\u001b[39m\u001b[39mkwargs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_7\u001b[39m():\n\u001b[0;32m    116\u001b[0m     \u001b[39mreturn\u001b[39;00m ()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileih5hpr1i.py:111\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__input_processing.<locals>.else_body_5\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39melse_body_4\u001b[39m():\n\u001b[0;32m    110\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mlen\u001b[39;49m), (ag__\u001b[39m.\u001b[39;49mld(kwargs)[\u001b[39m'\u001b[39;49m\u001b[39mkwargs_call\u001b[39;49m\u001b[39m'\u001b[39;49m],), \u001b[39mNone\u001b[39;49;00m, fscope) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m, if_body_4, else_body_4, get_state_4, set_state_4, (), \u001b[39m0\u001b[39;49m)\n\u001b[0;32m    112\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(kwargs)\u001b[39m.\u001b[39mpop, (\u001b[39m'\u001b[39m\u001b[39mkwargs_call\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileih5hpr1i.py:107\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__input_processing.<locals>.else_body_5.<locals>.if_body_4\u001b[1;34m()\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mif_body_4\u001b[39m():\n\u001b[1;32m--> 107\u001b[0m     \u001b[39mraise\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mValueError\u001b[39;00m), (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe following keyword arguments are not supported by this model: \u001b[39m\u001b[39m{\u001b[39;00mag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mlist\u001b[39m),\u001b[39m \u001b[39m(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(kwargs)[\u001b[39m'\u001b[39m\u001b[39mkwargs_call\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mkeys,\u001b[39m \u001b[39m(),\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m,\u001b[39m \u001b[39mfscope),),\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m,\u001b[39m \u001b[39mfscope)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"d:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\transformers\\trainer_tf.py\", line 711, in distributed_training_steps  *\n        self.args.strategy.run(self.apply_gradients, inputs)\n    File \"d:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\transformers\\trainer_tf.py\", line 653, in apply_gradients  *\n        gradients = self.training_step(features, labels, nb_instances_in_global_batch)\n    File \"d:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\transformers\\trainer_tf.py\", line 636, in training_step  *\n        per_example_loss, _ = self.run_model(features, labels, True)\n    File \"d:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\transformers\\trainer_tf.py\", line 758, in run_model  *\n        outputs = self.model(features, labels=labels, training=training)[:2]\n    File \"d:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 903, in run_call_with_unpacked_inputs  *\n        unpacked_inputs = input_processing(func, config, **fn_args_and_kwargs)\n    File \"d:\\Projekte\\Python\\Tensorflow\\Komposition-eines-Musikstuecks-mittels-Neuronaler-Netze\\.venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 499, in input_processing  *\n        raise ValueError(\n\n    ValueError: The following keyword arguments are not supported by this model: ['labels'].\n"
     ]
    }
   ],
   "source": [
    "from model import MukkeBudeTransformer\n",
    "from mapping import MusicMapping\n",
    "import utils\n",
    "import music21 as m21\n",
    "\n",
    "mapping = MusicMapping.create()\n",
    "paths = m21.corpus.getComposer('bach')\n",
    "song = utils.read_single_from_corpus(paths[0])\n",
    "encoded_song = utils.to_polyphonic_encoding(song, mapping)\n",
    "print(encoded_song)\n",
    "model = MukkeBudeTransformer()\n",
    "data = utils.create_train_data(encoded_song)\n",
    "# train = utils.split_to_training_data(mapping.textify(encoded_song))\n",
    "model.train(data, len(mapping))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9052d763eeeacaf5da08a840d251d734fa4deea148f97d28b11062c8c516292f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
